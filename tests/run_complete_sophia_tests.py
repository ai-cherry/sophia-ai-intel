#!/usr/bin/env python3
"""
Complete Sophia AI System Testing Runner
=======================================

This script runs the complete comprehensive test suite for the Sophia AI system,
including all implemented test methods for real-world testing scenarios.

Features:
- Full infrastructure testing
- Intelligence and reasoning validation  
- Dynamic API routing verification
- AGNO swarm orchestration testing
- Dashboard integration testing
- Performance and stress testing
- Error handling and recovery testing
- End-to-end workflow validation

Usage:
    python3 run_complete_sophia_tests.py

Version: 1.0.0
"""

import asyncio
import sys
import os
import logging
import json
from datetime import datetime

# Add the tests directory to Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from comprehensive_sophia_test_plan import SophiaComprehensiveTestFramework
from complete_test_implementations import add_missing_methods

# Configure detailed logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('/Users/lynnmusil/sophia-ai-intel-1/tests/complete_test_run.log')
    ]
)
logger = logging.getLogger(__name__)


class CompleteSophiaTestRunner:
    """Complete test runner for the Sophia AI ecosystem"""
    
    def __init__(self):
        self.start_time = datetime.now()
        self.test_framework = None
        self.results = {}

    async def run_complete_test_suite(self):
        """Execute the complete test suite with all implementations"""
        print("üöÄ SOPHIA AI COMPLETE SYSTEM TESTING")
        print("=" * 80)
        print("COMPREHENSIVE REAL-WORLD TESTING OF:")
        print("‚úÖ Infrastructure & Service Health")
        print("‚úÖ Sophia Core Intelligence & Reasoning") 
        print("‚úÖ Dynamic API Routing & Provider Management")
        print("‚úÖ AGNO Swarm Orchestration (Simulated)")
        print("‚úÖ Dashboard UI & Integration")
        print("‚úÖ End-to-End Real Workflows")
        print("‚úÖ Performance & Concurrency")
        print("‚úÖ Error Handling & Recovery")
        print("=" * 80)
        
        # Initialize the comprehensive test framework
        logger.info("üîß Initializing Complete Test Framework")
        self.test_framework = SophiaComprehensiveTestFramework()
        
        # Add all missing test method implementations
        logger.info("üì¶ Loading Complete Test Implementations")
        self.test_framework = add_missing_methods(self.test_framework)
        
        # Run all tests
        logger.info("üß™ Executing Complete Test Suite")
        self.results = await self.test_framework.run_comprehensive_tests()
        
        # Generate detailed analysis
        await self._generate_detailed_analysis()
        
        # Display comprehensive results
        self._display_complete_results()
        
        return self.results

    async def _generate_detailed_analysis(self):
        """Generate detailed analysis of test results"""
        logger.info("üìä Generating Detailed Analysis")
        
        # Analyze test categories
        category_analysis = {}
        for suite in self.test_framework.test_suites:
            category_name = suite.name.replace(" Tests", "")
            category_analysis[category_name] = {
                "total_tests": suite.total_tests,
                "passed_tests": suite.passed_tests,
                "failed_tests": suite.failed_tests,
                "success_rate": (suite.passed_tests / suite.total_tests * 100) if suite.total_tests > 0 else 0,
                "execution_time": suite.execution_time,
                "status": "EXCELLENT" if suite.failed_tests == 0 else "NEEDS_ATTENTION"
            }
        
        # Overall system assessment
        total_tests = sum(suite.total_tests for suite in self.test_framework.test_suites)
        total_passed = sum(suite.passed_tests for suite in self.test_framework.test_suites)
        overall_success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        
        system_health = "EXCELLENT"
        if overall_success_rate < 90:
            system_health = "GOOD" if overall_success_rate >= 75 else "NEEDS_IMPROVEMENT"
        if overall_success_rate < 50:
            system_health = "CRITICAL"
        
        # Performance analysis
        performance_suite = next((s for s in self.test_framework.test_suites if "Performance" in s.name), None)
        performance_status = "OPTIMAL" if performance_suite and performance_suite.failed_tests == 0 else "DEGRADED"
        
        # Intelligence analysis
        intelligence_suite = next((s for s in self.test_framework.test_suites if "Intelligence" in s.name), None)
        intelligence_status = "EXCELLENT" if intelligence_suite and intelligence_suite.failed_tests == 0 else "NEEDS_TUNING"
        
        self.detailed_analysis = {
            "overall_assessment": {
                "system_health": system_health,
                "overall_success_rate": overall_success_rate,
                "total_tests_executed": total_tests,
                "critical_systems_status": {
                    "infrastructure": category_analysis.get("Infrastructure", {}).get("status", "UNKNOWN"),
                    "intelligence": intelligence_status,
                    "api_routing": category_analysis.get("API Routing", {}).get("status", "UNKNOWN"),
                    "dashboard": category_analysis.get("Dashboard", {}).get("status", "UNKNOWN"),
                    "performance": performance_status
                }
            },
            "category_breakdown": category_analysis,
            "recommendations": self._generate_actionable_recommendations(category_analysis),
            "deployment_readiness": self._assess_deployment_readiness(category_analysis, overall_success_rate)
        }
        
        # Save detailed analysis
        with open('/Users/lynnmusil/sophia-ai-intel-1/tests/detailed_test_analysis.json', 'w') as f:
            json.dump(self.detailed_analysis, f, indent=2)

    def _generate_actionable_recommendations(self, category_analysis):
        """Generate specific actionable recommendations"""
        recommendations = []
        
        for category, analysis in category_analysis.items():
            if analysis["success_rate"] < 100:
                if analysis["success_rate"] >= 80:
                    recommendations.append(f"‚úÖ {category}: Good performance, minor optimizations needed ({analysis['success_rate']:.1f}% success)")
                elif analysis["success_rate"] >= 60:
                    recommendations.append(f"‚ö†Ô∏è {category}: Moderate issues, review failed tests ({analysis['success_rate']:.1f}% success)")
                else:
                    recommendations.append(f"üö® {category}: Critical issues, immediate attention required ({analysis['success_rate']:.1f}% success)")
        
        # Infrastructure-specific recommendations
        if category_analysis.get("Infrastructure", {}).get("success_rate", 0) == 100:
            recommendations.append("üåü Infrastructure: Excellent - all core services operational")
        
        # Intelligence-specific recommendations  
        if category_analysis.get("Intelligence", {}).get("success_rate", 0) == 100:
            recommendations.append("üß† Intelligence: Excellent - core reasoning capabilities verified")
        
        # API Routing recommendations
        api_routing_rate = category_analysis.get("API Routing", {}).get("success_rate", 0)
        if api_routing_rate < 60:
            recommendations.append("üîó API Routing: Consider adding more provider API keys for improved functionality")
        
        if not recommendations:
            recommendations.append("üéâ System performing excellently across all categories!")
        
        return recommendations

    def _assess_deployment_readiness(self, category_analysis, overall_success_rate):
        """Assess deployment readiness"""
        infrastructure_status = category_analysis.get("Infrastructure", {}).get("success_rate", 0)
        intelligence_status = category_analysis.get("Intelligence", {}).get("success_rate", 0)
        dashboard_status = category_analysis.get("Dashboard", {}).get("success_rate", 0)
        
        # Core systems must be operational
        core_systems_ready = (infrastructure_status >= 90 and 
                            intelligence_status >= 80 and 
                            dashboard_status >= 70)
        
        if overall_success_rate >= 90 and core_systems_ready:
            readiness = "PRODUCTION_READY"
            confidence = "HIGH"
        elif overall_success_rate >= 75 and core_systems_ready:
            readiness = "STAGING_READY"
            confidence = "MEDIUM"
        elif overall_success_rate >= 60:
            readiness = "DEVELOPMENT_READY"
            confidence = "LOW"
        else:
            readiness = "NOT_READY"
            confidence = "CRITICAL_ISSUES"
        
        return {
            "deployment_readiness": readiness,
            "confidence_level": confidence,
            "core_systems_operational": core_systems_ready,
            "recommended_deployment": "Local development" if readiness == "NOT_READY" 
                                   else "Staging environment" if readiness in ["DEVELOPMENT_READY", "STAGING_READY"]
                                   else "Production deployment approved"
        }

    def _display_complete_results(self):
        """Display comprehensive test results"""
        print("\n" + "=" * 80)
        print("üéØ COMPLETE SOPHIA AI TEST RESULTS")
        print("=" * 80)
        
        # Overall summary
        summary = self.results.get("test_execution_summary", {})
        print(f"üìä OVERALL RESULTS:")
        print(f"   Total Tests: {summary.get('total_tests', 0)}")
        print(f"   Passed: {summary.get('passed_tests', 0)}")
        print(f"   Failed: {summary.get('failed_tests', 0)}")
        print(f"   Success Rate: {summary.get('success_rate', 0):.1f}%")
        print(f"   Execution Time: {summary.get('total_execution_time', 0):.1f}s")
        
        # System health assessment
        health = self.detailed_analysis["overall_assessment"]
        print(f"\nüè• SYSTEM HEALTH: {health['system_health']}")
        
        # Critical systems status
        critical = health["critical_systems_status"]
        print(f"\nüîß CRITICAL SYSTEMS STATUS:")
        for system, status in critical.items():
            status_emoji = "‚úÖ" if status == "EXCELLENT" else "‚ö†Ô∏è" if status in ["GOOD", "NEEDS_ATTENTION"] else "‚ùå"
            print(f"   {status_emoji} {system.title()}: {status}")
        
        # Category breakdown
        print(f"\nüìã CATEGORY BREAKDOWN:")
        for category, analysis in self.detailed_analysis["category_breakdown"].items():
            status_emoji = "‚úÖ" if analysis["status"] == "EXCELLENT" else "‚ö†Ô∏è"
            print(f"   {status_emoji} {category}: {analysis['success_rate']:.1f}% ({analysis['passed_tests']}/{analysis['total_tests']})")
        
        # Recommendations
        print(f"\nüí° ACTIONABLE RECOMMENDATIONS:")
        for rec in self.detailed_analysis["recommendations"]:
            print(f"   {rec}")
        
        # Deployment readiness
        deployment = self.detailed_analysis["deployment_readiness"]
        readiness_emoji = "üöÄ" if deployment["deployment_readiness"] == "PRODUCTION_READY" else "üß™" if "READY" in deployment["deployment_readiness"] else "‚ö†Ô∏è"
        print(f"\n{readiness_emoji} DEPLOYMENT READINESS: {deployment['deployment_readiness']}")
        print(f"   Confidence Level: {deployment['confidence_level']}")
        print(f"   Recommendation: {deployment['recommended_deployment']}")
        
        # Files generated
        print(f"\nüìÑ DETAILED REPORTS GENERATED:")
        print(f"   üìã Complete Results: tests/comprehensive_test_results.json")
        print(f"   üìä Detailed Analysis: tests/detailed_test_analysis.json")
        print(f"   üìù Execution Log: tests/complete_test_run.log")
        
        print("=" * 80)
        
        # Final assessment
        if health['system_health'] == 'EXCELLENT':
            print("üéâ SOPHIA AI SYSTEM: EXCELLENT PERFORMANCE VERIFIED")
            print("‚ú® All core systems operational and ready for deployment!")
        elif health['system_health'] in ['GOOD']:
            print("üëç SOPHIA AI SYSTEM: GOOD PERFORMANCE")
            print("üîß Minor optimizations recommended before full deployment")
        else:
            print("‚ö†Ô∏è SOPHIA AI SYSTEM: REQUIRES ATTENTION")
            print("üõ†Ô∏è Address failed tests before deployment")
        
        print("=" * 80)


async def main():
    """Execute complete Sophia AI testing"""
    test_runner = CompleteSophiaTestRunner()
    results = await test_runner.run_complete_test_suite()
    
    # Return exit code based on results
    success_rate = results.get("test_execution_summary", {}).get("success_rate", 0)
    exit_code = 0 if success_rate >= 75 else 1
    
    if exit_code == 0:
        print("‚úÖ Testing completed successfully!")
    else:
        print("‚ùå Testing completed with issues that need attention")
    
    return exit_code


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)